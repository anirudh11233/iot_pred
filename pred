import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# =========================
# CONFIG
# =========================

DATA_PATH = "C:\\Users\\Siri Chandana\\Desktop\\iot\\predictive_maintenance_dataset.csv"  # <- change to your filename

# Change these to match your CSV
FEATURE_COLUMNS = [
    "vibration",
    "acoustic",
    "temperature",
    "current",
    "IMF_1",
    "IMF_2",
    "IMF_3",
]
LABEL_COLUMN = "label"  # 0/1 for failure


# =========================
# HELPER FUNCTIONS
# =========================

def create_sequences(X, y, window_size=10):
    """
    Create sliding-window sequences for LSTM/GRU.
    X: 2D array (time, features)
    y: 1D array (time)
    Returns:
        X_seq: (samples, window_size, features)
        y_seq: (samples,)
    """
    Xs, ys = [], []
    for i in range(len(X) - window_size):
        Xs.append(X[i:i + window_size])
        ys.append(y[i + window_size])
    return np.array(Xs), np.array(ys)


def evaluate_model(name, model, X_train, y_train, X_test, y_test, is_sequence=False,
                   sequence_epochs=15, sequence_batch_size=32):
    """
    Train and evaluate a model; return metrics in dict form.
    For Keras sequence models, set is_sequence=True.
    """
    if is_sequence:
        # Keras model (LSTM/GRU)
        early_stop = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        history = model.fit(
            X_train,
            y_train,
            epochs=sequence_epochs,
            batch_size=sequence_batch_size,
            validation_split=0.2,
            callbacks=[early_stop],
            verbose=0,
        )
        y_pred_prob = model.predict(X_test).ravel()
        y_pred = (y_pred_prob > 0.5).astype(int)
    else:
        # Classical ML models
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        if hasattr(model, "predict_proba"):
            y_pred_prob = model.predict_proba(X_test)[:, 1]
        else:
            # If no predict_proba (rare here), approximate with predictions
            y_pred_prob = y_pred.astype(float)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_test, y_pred_prob)

    print(f"\n=== {name} ===")
    print(f"Accuracy : {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall   : {recall:.4f}")
    print(f"F1-score : {f1:.4f}")
    print(f"ROC-AUC  : {roc_auc:.4f}")

    return {
        "Model": name,
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1,
        "ROC-AUC": roc_auc,
    }


# =========================
# MAIN WORKFLOW
# =========================

def main():
    # 1) Load data
    df = pd.read_csv(DATA_PATH)

    # Basic sanity check
    missing_cols = [c for c in FEATURE_COLUMNS + [LABEL_COLUMN] if c not in df.columns]
    if missing_cols:
        raise ValueError(f"These columns are missing in CSV: {missing_cols}")

    X = df[FEATURE_COLUMNS].values
    y = df[LABEL_COLUMN].values

    # 2) Train-test split (simple random split; for real PdM use time-based split)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # 3) Scaling for classical ML + sequence models
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 4) Classical models (tabular)
    results = []

    # Logistic Regression
    lr = LogisticRegression(max_iter=1000)
    results.append(
        evaluate_model(
            "Logistic Regression",
            lr,
            X_train_scaled,
            y_train,
            X_test_scaled,
            y_test,
            is_sequence=False,
        )
    )

    # Random Forest
    rf = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1
    )
    results.append(
        evaluate_model(
            "Random Forest",
            rf,
            X_train,
            y_train,
            X_test,
            y_test,
            is_sequence=False,
        )
    )

    # XGBoost
    xgb_model = xgb.XGBClassifier(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="logloss",
        random_state=42,
        n_jobs=-1,
    )
    results.append(
        evaluate_model(
            "XGBoost",
            xgb_model,
            X_train,
            y_train,
            X_test,
            y_test,
            is_sequence=False,
        )
    )

    # 5) Sequence models (LSTM/GRU)
    # For simplicity, we create sequences from the SCALED train & test (not mixing).
    WINDOW_SIZE = 10

    # Important: sequences reduce the number of samples; keep that in mind.
    X_seq_train, y_seq_train = create_sequences(X_train_scaled, y_train, WINDOW_SIZE)
    X_seq_test, y_seq_test = create_sequences(X_test_scaled, y_test, WINDOW_SIZE)

    input_shape = (X_seq_train.shape[1], X_seq_train.shape[2])  # (window_size, num_features)

    # LSTM
    lstm_model = Sequential([
        LSTM(64, input_shape=input_shape),
        Dropout(0.3),
        Dense(1, activation="sigmoid"),
    ])
    lstm_model.compile(
        loss="binary_crossentropy",
        optimizer="adam",
        metrics=["accuracy"],
    )

    results.append(
        evaluate_model(
            "LSTM",
            lstm_model,
            X_seq_train,
            y_seq_train,
            X_seq_test,
            y_seq_test,
            is_sequence=True,
            sequence_epochs=20,
            sequence_batch_size=64,
        )
    )

    # GRU
    gru_model = Sequential([
        GRU(64, input_shape=input_shape),
        Dropout(0.3),
        Dense(1, activation="sigmoid"),
    ])
    gru_model.compile(
        loss="binary_crossentropy",
        optimizer="adam",
        metrics=["accuracy"],
    )

    results.append(
        evaluate_model(
            "GRU",
            gru_model,
            X_seq_train,
            y_seq_train,
            X_seq_test,
            y_seq_test,
            is_sequence=True,
            sequence_epochs=20,
            sequence_batch_size=64,
        )
    )

    # 6) Summary table
    results_df = pd.DataFrame(results)
    print("\n\n===== SUMMARY TABLE =====")
    print(results_df.to_string(index=False))

    # Optionally save to CSV for paper
    results_df.to_csv("model_results_summary.csv", index=False)
    print("\nSaved summary to model_results_summary.csv")


if __name__ == "__main__":
    main()
