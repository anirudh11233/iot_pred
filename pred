import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# =========================
# CONFIG
# =========================

DATA_PATH = r"C:\Users\saian\OneDrive\Desktop\iot\iot_pred\predictive_maintenance_dataset.csv"

FEATURE_COLUMNS = [
    "vibration",
    "acoustic",
    "temperature",
    "current",
    "IMF_1",
    "IMF_2",
    "IMF_3",
]
LABEL_COLUMN = "label"


# =========================
# HELPERS
# =========================

def create_sequences(X, y, window_size=10):
    Xs, ys = [], []
    for i in range(len(X) - window_size):
        Xs.append(X[i:i + window_size])
        ys.append(y[i + window_size])
    return np.array(Xs), np.array(ys)


def safe_roc_auc(y_true, y_prob):
    if len(np.unique(y_true)) < 2:
        return np.nan
    return roc_auc_score(y_true, y_prob)


def evaluate_model(name, model, X_train, y_train, X_test, y_test, is_sequence=False, class_weight=None):
    if is_sequence:
        early_stop = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        model.fit(
            X_train,
            y_train,
            epochs=20,
            batch_size=64,
            validation_split=0.2,
            callbacks=[early_stop],
            class_weight=class_weight,
            verbose=0,
        )
        y_prob = model.predict(X_test).ravel()
        y_pred = (y_prob > 0.5).astype(int)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

    return {
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, zero_division=0),
        "Recall": recall_score(y_test, y_pred, zero_division=0),
        "F1-score": f1_score(y_test, y_pred, zero_division=0),
        "ROC-AUC": safe_roc_auc(y_test, y_prob),
    }


# =========================
# MAIN
# =========================

def main():
    df = pd.read_csv(DATA_PATH)

    # Optional safety
    df = df.dropna()

    X = df[FEATURE_COLUMNS].values
    y = df[LABEL_COLUMN].values

    # ðŸ”¥ TIME-BASED SPLIT (PdM correct)
    split_idx = int(0.7 * len(df))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Class imbalance ratio
    neg, pos = np.bincount(y_train)
    scale_pos_weight = neg / pos

    print("Class distribution (train):", np.bincount(y_train))
    print("Scale pos weight:", scale_pos_weight)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    results = []

    # -------------------------
    # Logistic Regression
    # -------------------------
    results.append(
        evaluate_model(
            "Logistic Regression",
            LogisticRegression(max_iter=1000, class_weight="balanced"),
            X_train_scaled, y_train,
            X_test_scaled, y_test,
        )
    )

    # -------------------------
    # Random Forest
    # -------------------------
    results.append(
        evaluate_model(
            "Random Forest",
            RandomForestClassifier(
                n_estimators=200,
                random_state=42,
                class_weight="balanced",
                n_jobs=-1
            ),
            X_train, y_train,
            X_test, y_test,
        )
    )

    # -------------------------
    # XGBoost
    # -------------------------
    results.append(
        evaluate_model(
            "XGBoost",
            xgb.XGBClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                eval_metric="logloss",
                scale_pos_weight=scale_pos_weight,
                random_state=42,
            ),
            X_train, y_train,
            X_test, y_test,
        )
    )

    # -------------------------
    # SEQUENCE MODELS
    # -------------------------
    WINDOW = 10
    X_seq_train, y_seq_train = create_sequences(X_train_scaled, y_train, WINDOW)
    X_seq_test, y_seq_test = create_sequences(X_test_scaled, y_test, WINDOW)

    seq_class_weight = {
        0: 1.0,
        1: (y_seq_train == 0).sum() / (y_seq_train == 1).sum()
    }

    input_shape = (X_seq_train.shape[1], X_seq_train.shape[2])

    # -------------------------
    # LSTM
    # -------------------------
    lstm = Sequential([
        LSTM(64, input_shape=input_shape),
        Dropout(0.3),
        Dense(1, activation="sigmoid"),
    ])
    lstm.compile(loss="binary_crossentropy", optimizer="adam")

    results.append(
        evaluate_model(
            "LSTM",
            lstm,
            X_seq_train, y_seq_train,
            X_seq_test, y_seq_test,
            is_sequence=True,
            class_weight=seq_class_weight
        )
    )

    # -------------------------
    # GRU
    # -------------------------
    gru = Sequential([
        GRU(64, input_shape=input_shape),
        Dropout(0.3),
        Dense(1, activation="sigmoid"),
    ])
    gru.compile(loss="binary_crossentropy", optimizer="adam")

    results.append(
        evaluate_model(
            "GRU",
            gru,
            X_seq_train, y_seq_train,
            X_seq_test, y_seq_test,
            is_sequence=True,
            class_weight=seq_class_weight
        )
    )

    # -------------------------
    # RESULTS
    # -------------------------
    results_df = pd.DataFrame(results)
    print("\n===== MODEL COMPARISON (IMBALANCE FIXED) =====")
    print(results_df)

    results_df.to_csv("model_results_summary.csv", index=False)
    print("\nSaved: model_results_summary.csv")


if __name__ == "__main__":
    main()
